# MVP-Critical Fixes from Codex Review

Only issues that will **break or block the v1 build** if not addressed. Everything else (v2 WebSocket, event persistence, multi-consumer fanout, secret redaction, correlated errors, health-checks) is deferred.

---

## 1. Tool Registry: No Auto-Discovery

**Problem:** Design says `register_mcp_server()` "discovers tools on registration." The SDK doesn't support this. Without it, the planner won't know what MCP tools exist, and the validator can't check tool names.

**Fix:** Users explicitly register tools with name + description when adding an MCP server. No auto-discovery.

```python
registry.register_mcp_server(
    server_name="github",
    config=MCPServerConfig(command="npx", args=["-y", "@modelcontextprotocol/server-github"]),
    tools=[
        ToolInfo(name="mcp__github__list_issues", description="List GitHub issues"),
        ToolInfo(name="mcp__github__create_issue", description="Create a GitHub issue"),
    ]
)
```

---

## 2. Tool Naming: `human_input_tool` / `ask_duckie` Don't Exist in SDK

**Problem:** SPEC uses `human_input_tool` and `ask_duckie`. SDK built-in is `AskUserQuestion`. `ask_duckie` is not a real tool at all. If the planner references these names, the executor will fail.

**Fix:**
- Map `human_input_tool` → `AskUserQuestion` in the planner prompt and tool registry
- Drop `ask_duckie` from the system entirely (it's domain-specific to the original SPEC context, not relevant to our generic implementation)
- Update the planner prompt to use `AskUserQuestion` as the Tier-3 implicit tool

---

## 3. AgentRunner: How to Extract YAML from SDK Message Stream

**Problem:** SDK `query()` yields a stream of `AssistantMessage`, `ToolUseBlock`, `ResultMessage`, etc. Design doesn't specify how to pull the final YAML output from this stream.

**Fix:** Define the extraction rule:
1. Collect all `AssistantMessage` blocks from the stream
2. Take the last `TextBlock` content from the final `AssistantMessage`
3. Pass it through `YAMLCleaner.parse()`
4. If `ResultMessage` has `subtype == "error"`, treat as agent failure

---

## 4. Voting: Can Loop Forever

**Problem:** `FirstToKVoter` has no `max_samples`. If no answer pulls ahead by K, it runs agents indefinitely — infinite cost and hang.

**Fix:** Add to `TaskConfig`:
```python
max_voting_samples: int = 10   # hard cap on total samples per step
```
If no winner after `max_voting_samples`, fail the step.

Also: formally define First-to-K as `leader_count - runner_up_count >= K` (not "over all others").

---

## 5. Voting: Exact String Match Splits Votes

**Problem:** Majority voter uses exact serialized output match. Two semantically identical YAML outputs with different key ordering or whitespace will count as different answers, splitting votes and preventing consensus.

**Fix:** Canonicalize before comparing:
1. Parse YAML to dict
2. Sort keys recursively
3. Normalize whitespace/formatting
4. Serialize back to canonical string
5. Compare canonical strings

---

## 6. `output_schema` Is a Free-Form String

**Problem:** `output_schema: "{incident_id: string, severity: int}"` is just a string. The red-flagger can't programmatically validate agent output against it. The voter can't use it for canonicalization. But different use cases will produce wildly different output shapes — we can't be too rigid.

**Fix:** Treat `output_schema` as a **hint, not a contract**. The approach:

1. **Planner writes it as a free-form string** (no change — LLMs are good at this)
2. **Validator checks it exists and is non-empty** (deterministic, pass/fail)
3. **Red-flagger uses it loosely** — agent output must be valid YAML and must be a dict/object (not raw text). That's it. No field-level enforcement. The schema string is passed as context to help the agent, not to gate its output.
4. **Voter canonicalizes the full output** — parse to dict, sort keys recursively, serialize. Compare the whole thing. Don't try to extract only declared fields.

**Why loose:** Different tasks will return different structures — a code search returns a list of files, a data lookup returns a nested object, a computation returns a single value. Forcing all outputs through a rigid schema parser will break on edge cases and add complexity with little value. The LLM already knows what shape to produce from the `output_schema` hint in its prompt. If the output is valid YAML and a dict, that's sufficient for the pipeline to continue.

---

## 7. Conditional Step: How Does Executor Get the Next Step Number?

**Problem:** For `conditional_step`, the agent evaluates logic and decides the next step. But the design doesn't specify how the executor extracts that integer from the agent's output.

**Fix:** Conditional steps must output: `{next_step: int, reason: string}`. Executor parses this via `YAMLCleaner`, reads `next_step` field, routes accordingly. If `next_step` is missing or invalid, fail the step.

---

## 8. Plan Schema: `plan` vs `steps` Mismatch

**Problem:** SPEC YAML has `reasoning` + `plan` as top-level keys. Design `Plan` dataclass has `reasoning` + `steps`. Parser needs to know which key to read.

**Fix:** Parser maps `plan` → `steps` when loading. Single line fix, but must be explicit or the planner output won't parse.

---

## 9. Missing Config: Step Timeout and Retry Limit

**Problem:** Error handling table says "retry planner" but `TaskConfig` has no per-step retry fields. Without these, the executor has no bounds.

**Fix:** Add to `TaskConfig`:
```python
step_max_retries: int = 2          # retries per step (before voting kicks in)
```

---

## 10. Context Resolver: Keep It Simple, Pass Whole Outputs

**Problem:** SPEC uses `step_2_output.signals[0]` but the design doesn't define how array indexing or nested paths work. Building a full path-resolution engine (dot notation + array indexing + nested keys) adds complexity and risks extracting the wrong thing.

**Fix:** Don't resolve paths at all. Pass whole step outputs to the agent and let the LLM extract what it needs.

How it works:
1. `input_variables` lists which step outputs this step needs (e.g., `[step_0_output.user_id, step_2_output.signals]`)
2. Context resolver reads only the **step name** from each input variable (everything before the first `.`) — e.g., `step_0_output`, `step_2_output`
3. It injects the **full output dict** of each referenced step into the agent's prompt as YAML
4. The agent's `task_description` already tells it which fields to use (e.g., "Use `step_0_output.user_id`")
5. The LLM reads the full output and extracts the right values itself

**Why this is better:**
- No path-parsing bugs (wrong index, missing key, type mismatch)
- No code to maintain for edge cases (nested arrays, escaped keys, optional fields)
- LLMs are good at reading structured data and pulling out what they need
- If the planner writes a slightly wrong path, the LLM still finds the data
- `input_variables` still serves as documentation of what the step depends on

**Trade-off:** Slightly larger prompts (full step output vs extracted value). Acceptable for v1 at 100-user scale.

---

## Summary: 10 Fixes for MVP

| # | Fix | Effort |
|---|-----|--------|
| 1 | Explicit MCP tool registration (no auto-discovery) | Small — change registry API |
| 2 | Map `human_input_tool` → `AskUserQuestion`, drop `ask_duckie` | Small — prompt + registry |
| 3 | Define YAML extraction rule from SDK message stream | Small — AgentRunner logic |
| 4 | Add `max_voting_samples` to prevent infinite loop | Small — config + voter |
| 5 | Canonicalize YAML before vote comparison | Medium — voter utility |
| 6 | Treat `output_schema` as hint, not contract (loose validation) | Small — red-flagger just checks valid YAML + dict |
| 7 | Define conditional step output contract (`next_step` field) | Small — executor logic |
| 8 | Map `plan` → `steps` in YAML parser | Trivial |
| 9 | Add `step_max_retries` to config | Small |
| 10 | Pass whole step outputs to agents (no path resolution) | Small — simpler than building a resolver |
